## 论文题目
Distributed Representations of Words and Phrases and their Compositionality
## 作者
Tomas Mikolov
## 摘要
最近推出的连续Skip-gram模型是学习高质量分布式矢量表示的有效方法，可以捕获大量精确的句法和语义单词关系。
在本文中，我们提出了几个扩展，可以提高向量的质量和训练速度。 通过对频繁单词进行二次取样，我们获得了显着
的加速，并且还学习了更多的常规单词表示。 我们还描述了一种称为负采样的分层softmax的简单替代方案。 单词
表示的固有局限性是它们对单词顺序的漠不关心以及它们无法表示惯用语。 例如，“加拿大”和“空气”的含义不能轻易组合以获得“加拿大航空”。
在这个例子的推动下，我们提出了一种在文本中查找短语的简单方法，并表明可以为数百万个短语学习好的矢量表示。
## 论文地址
[论文](https://arxiv.org/abs/1310.4546)点击下载
## 推荐理由
本文是word2vec的后续，基于之前提出的两种词向量的学习模型，又提出了两种能够提高运算效率和学习结果的准确度的方法，
层次化softmax和负采样．同时提出了训练短语的方法，能够更好地获取单词的语义信息．  
word2vec为后来的doc2vec, node2vec等模型提供了基础，是很常用的词向量的训练方法．
